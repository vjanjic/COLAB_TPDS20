
%\ty{\section{Energy-Aware Scheduler Extension on ARMv8 Architecture}}

%\ty{Furthermore, the energy consumption is a critical issue for multi-program co-execution on asymmetric multi-core processors. Different threads from application programs are variant between different type of cores. Followed by the prototype on GEM5 simulator, We fully-implement our COLAB scheduler on Linux kernel v3.16 with an energy-aware extension targeting ARMv8 architecture.} 

\begin{comment}
\begin{table}
  \caption{\ty{Selected ARMv8 PMUs and the Speedup and Energy Models}}
  \center
  \label{pca_arm_sp}
   \scalebox{0.8}{
   \begin{tabular}{p{0.8cm} | p{3.6cm} | p {5.4cm} c c c}
  \hline
     Index&  Name& Description \cite{ARMA57} \\
    \hline
     A: & L1D\_CACHE\_REFILL & \# L1 data cache refill\\
     B: & L1D\_TLB\_REFILL & \# Attributable L1 data TLB refill\\
     C: & L1D\_CACHE\_WB & \# Attributable L1 data cache write-back\\
     D: & L2D\_CACHE & \# L2 data cache access\\
     E: & L2D\_CACHE\_WB & \# Attributable L2 data cache write-back\\
     F: & BUS\_ACCESS & \# Bus access\\
     \hline
     G: & Cycle & \# Processor clock cycle\\
     \hline
     \multicolumn{3}{c}{Linear predictive speedup model}\\
     \hline
    \multicolumn{3}{c}{2.6185+((-7.8618*A)+(-25.7769*B)+}
    \\
    \multicolumn{3}{c}{-34.2781*C)+(11.2167*D)+(44.3879*E)+(-10.1942*F))/G}\\
 \hline
  \multicolumn{3}{c}{Linear predictive energy model}\\
     \hline
    \multicolumn{3}{c}{8.3818+((-2.5524*A)+(-9.5247*B)+}
    \\
    \multicolumn{3}{c}{(-11.7908*C)+(3.481476*D)+(18.854182*E)+(-4.9607*F))/G}\\
 \hline
  \end{tabular}}
\end{table}
\end{comment}

%\ty{\subsection{Machine Learning based ARMv8 Performance and Energy Models}}

%\ty{The abstract offline PMU selection process is shown in Fig.~\ref{arm_model}(a). We first run all benchmarks compiled in single-threaded version on either 4 Cortex-A73 or 4 Cortex-A53 configuration. We only record common PMUs used for both Cortex-A73 and Cortex-A53, together with each thread's relative speedup and energy consumption. We ignore the PMUs for which their value keep to be zero, such as \texttt{SW\_INCR} (Instruction architecturally executed, Condition code check pass, software increment). For the remaining PMUs, we then apply the PCA technique to select the most important 6-PMU-combination. For example, it is easy to know that the performance and energy will be more dominated by the total data movement than some branch behaviours. As a result, although \texttt{BR\_IMMED\_RETIRED} is a meaningful PMU which counts all immediate branch instructions that are architecturally executed, but it is not selected after the PCA process as there are more important PMUs such as \texttt{L1D\_CACHE\_REFILL} (L1 data cache refill) and \texttt{L2\_CACHE} (L2 data cache access). }

%\ty{The trained PMU-based ARMv8 performance and energy models are shown in Table~\ref{arm_model}(b). In real ARM system validation, we use the clock cycle number to replace the GEM5 committed instructions as it is more stable recorded by a specialized register on the test board.}

%\ty{We validate the accuracy of our trained models on each benchmark as shown in Fig.~\ref{arm_model}(c). The average error cross all benchmark for the performance model is around 7\% while for the energy model is around 1.5\%.}
%The range of these errors are within the normal performance and energy variance of the test board, so we can claim that the trained models are accuracy enough to predicate the system behaviours.}

\begin{figure}[!t]
\label{alg:2}
\begin{algorithmic}[]
\STATE \textbf{\_core\_alloctor\_e(thread\_struct\ $t$)}:
\STATE \ \ \textbf{if} t.high\_hyper
\STATE \ \ \ \ \textbf{return} rr\_allocator\_(big\_cores)
\STATE \ \ \textbf{if} {t.low\_hyper \& t.low\_block}
\STATE \ \ \ \ \textbf{return} rr\_allocator\_(little\_cores)
\STATE \ \ \textbf{else return} rr\_allocator\_(cores)
\end{algorithmic}
\caption{Energy-aware COLAB Extension Algorithm}
\end{figure}

\textbf{\textit{Energy-aware Algorithm Extension}}
Until now, we have only considered how to make scheduling decisions to maximize overall performance. Given the different performance-power characteristics of different cores in an AMP system, scheduling decision can be also used to maximize energy efficiency or to achieve a certain trade-off between performance and energy. So, we extend the original scheduler to consider both optimization targets. We use a hyper-heuristic to guide the core assignment of a thread $t$:
$$ hyper\_(t) = w_{s}*speedup(t) + w_{e}*energy(t) $$
where $w_{s}$ and $w_{e}$ are the trained weights of the speedup factor and energy scaling factor, respectively; $energy(t)$ is the energy scaling factor for the thread $t$ between big and little cores, practically the ratio of consumed energy for the same amount of work between the two core types. We select the weights of this hyper-heuristic using a similar approach as for the performance and energy models. We collect data for our training programs running on the two core types and then use hierarchical regression to obtain the best trade-off between performance gain and energy saving. \fixme{PP: The previous sentence is incorrect. There is not a single trade-off between performance and energy, there is an infinite amount of potential trade-offs. Which trade-off did we target? What are we trying to maximize? What kind of training data did we use?}

The extended energy-aware core allocator \texttt{rr\_allocator\_e()} for the COLAB scheduling algorithm is presented in Figure~\ref{alg:2}. The hyper-heuristic essentially replaces the speedup factor in the original \texttt{rr\_allocator\_()}. This allocation policy avoids placing threads on the big cores if their performance does not improve significantly or their energy efficiency deteriorates, instead of considering performance only.
%When the energy-aware extension is enabled, threads labeled by this hyper-heuristic involve both the performance and energy concerns instead of the predicated speedup only. Under this hyper-heuristic, core allocator can provide the energy-aware solutions. It only allocates high speedup and non-energy-intensive threads to big cores, while keeps energy-intensive threads on little cores even they also have some kind of speedup. 

\subsection{Scheduling Overhead Analysis}
The overhead of the scheduling algorithms themselves is negligible. Collecting the information needed to make our scheduling decisions has some overhead though. Our performance and energy models require accumulating per task performance event counts from seven PMUs. This means that every time we context switch we have to access these PMUs. Reading each one takes 4 cycles on the Cortex-A53 and 14 cycles on the Cortex-A73. Since we need to read seven units, the worst case overhead is <100 cycles per context switch. The typical time between context switches for a single thread is orders of magnitude more than a hundred cycles, so the cost of reading the PMUs should be negligible too.

Identifying blocking threads requires instrumentation in the latency-sensitive futex code. Still, the code we added is short and relatively infrequently called, so we were not able to observe any negative effects on performance. \fixme{PP: Have we measured this overhead?}

Finally, we need to use this information to label all threads as high/low speedup, high/low energy scaling, and high/low blocking. This does requires some processing time but we purposefully kept the model simple, a linear regression with seven parameters. Additionally, labels are updated only once every 10 msec, so the total observed overhead is low. \fixme{PP: Do we have a measurement for this?}

