\section{Background and Related Work}
\label{rw}

\begin{table}[htbp]
  \caption{Qualitative Analysis on Related Work}
  \begin{center}
  \label{rwt}
  \scalebox{1.0}{
   \begin{tabular}{p{2.5cm} p{0.95cm} p{0.95cm} p{0.95cm} p{1.05cm}}
   \hline
     %\toprule[1pt]
     Approaches&Core Sens.&Fair- ness&Bottle- neck&Collabo- rative\\
     %\toprule[1pt]
     \hline
    Kumar, et al \cite{kumar2004single} &$\checkmark$&&&\\
    Li, et al \cite{li2009efficient}&&$\checkmark$&&\\
    Suleman, et al. \cite{suleman2009accelerating}&&&$\checkmark$&\\
    Saez, et al. \cite{saez2012leveraging}&$\checkmark$&$\checkmark$&&\\
    Craeynest, et al. \cite{van2013fairness}&$\checkmark$&$\checkmark$&&\\
    Cao, et al. \cite{cao2012yin}&$\checkmark$&&&\\
    Joao, et al \cite{joao2013utility}&$\checkmark$&&$\checkmark$&\\
    ARM \cite{jeff2013big}&&&$\checkmark$&\\
    Kim, et al \cite{kim2018exploring}&$\checkmark$&$\checkmark$&\\
    Jibaja, et al \cite{jibaja2016portable}&$\checkmark$&$\checkmark$&$\checkmark$\\
    \textbf{COLAB}&$\checkmark$&$\checkmark$&$\checkmark$&$\checkmark$\\
    \hline    
%\bottomrule
  \end{tabular}}
  \end{center}
\end{table}

Single-ISA heterogeneous processors allow for more efficient processing by using the right kind of core for each workload, while still relying on a single contract between hardware and software~\cite{kumar2004single,kumar2003single}. The only problem is deciding what "the right kind of core" is. For general purpose systems, where the mix of applications sharing the computational resources is not known at design time and changes rapidly, the optimal assignment of threads cannot be made statically. A scheduler needs to makes this decision at runtime.

The most direct approach, \emph{core sensitivity}, assigns threads to cores where they will experience the highest speedup. At its simplest, this might mean measuring the number of committed instructions per second on all core types and choosing the highest. In most cases though, this is inefficient. Existing approaches use performance models to predict the speedup from moving a thread to another core without having to execute the thread on that core. They first measure certain aspects of the thread's execution on its current core, \emph{ILP} and \emph{LLC miss rates} in Saez et al.~\cite{saez2012leveraging}, \emph{CPI stack}, \emph{ILP}, and \emph{MLP} in Craeynest et al.~\cite{van2012scheduling}, empirically selected performance counters in Jibaja et al~\cite{jibaja2016portable}. Then they use the performance model to estimate the effect of moving the thread elsewhere. The threads most sensitive to their placement are assigned to their preferred core type, the rest are assigned to any type of core.

Whole-program performance depends not only on accelerating core sensitive threads but also on accelerating the threads that slow down the program, the \emph{bottleneck and critical threads}. For example, previous work has shown the benefit from accelerating Amdahl's serial bottlenecks~\cite{Kumar:2005:HCM:1100859.1100890} and critical code sections~\cite{suleman2009accelerating}. Joao et al.~\cite{joao2012bottleneck,joao2013utility} further showed that functions that cause threads to wait above a certain threshold should be accelerated. Jibaja et al.~\cite{jibaja2016portable} similarly proposed finding bottleneck Java threads by measuring waiting time on contended locks.

On top of accelerating core sensitive and bottleneck threads, a general purpose AMP scheduler needs to \emph{maintain fairness}, that is to balance the processing resources given to each thread and process when they have to share these resources. Traditional fair schedulers balance only processing time, assuming that time is equally important on all cores. This is not true for AMPs. Li et al~\cite{li2007efficient} introduced some sense of fairness by loading each core proportionally to its processing power. Craeynest et al.~\cite{van2013fairness} instead proposed an \emph{equal progress} scheduler. It uses a performance model to express the progress of every thread in terms of the time required for the same progress on the small core. The scheduler then prioritizes threads so that the small core equivalent time of all threads is the same. Other heuristics have tried to maximize fairness on AMPs~\cite{zahedi2018amdahl,wang2016rebudget,kim2018exploring} but for restricted scenarios.

Only a small number of schedulers are designed to handle the general case of multi-threaded multi-programmed workloads. ARM Global Task Scheduler (GTS)~\cite{jeff2013big} focuses on energy efficiency. Threads run on low power cores unless they are servicing interrupts or have a high load. Core sensitivity is not a concern, while bottlenecks are only accelerated when they happen to be caused by high load threads. Kim and Huh~\cite{kim2018exploring} focuses only on core sensitivity and fairness. Inter-thread communication is not a concern. WASH~\cite{jibaja2016portable} is to our knowledge the only existing scheduler that handles core sensitivity, bottlenecks, and maintains fairness. Still, it is limited to only controlling affinity and requires the OS scheduler to independently dispatch threads to cores. As we see later, this lack of collaboration between the two subsystems is sub-optimal. In the rest of this paper, we use a WASH-like implementation that relies on the baseline Linux scheduler as our state-of-the-art. A qualitative comparison of the related work is shown in Table \ref{rwt}.