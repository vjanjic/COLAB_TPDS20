\section{Introduction}
\label{itr}

Balancing between performance and energy consumption is one of the central issues in designing new processors as over 90\% of all processors end up in embedded energy-limited devices, such as smartphones and IoT sensors. \emph{Heterogeneous systems}, that combine different processor types, provide energy-efficeint processing for different types of workloads~\cite{seeker2014measuring}. The first heterogeneous systems combined processors with different Instruction Set Architectures (ISA). More recently, single-ISA asymmetric multicore processors (AMPs) have gained popularity. Their advantages are obvious - because the processors share the same architecture, the decisions about what task/thread to map to what processor/core can be made at runtime by the OS thread scheduler, guided not only by the characteristics of the workload, but also by the runtime load of individual processors/cores. On the other hand, this introduces an extra degree of freedom to the scheduling problem, making it even more complex. As a result, efficient AMP scheduling has attracted a lot of attention in the literature~\cite{mittal2016survey}. The three main factors influencing the decisions of a general purpose AMP scheduler on a heterogeneous system are: 

\textbf{\textit{Core sensitivity:}} Each type of a core is designed to handle different kinds of workloads. For example, in ARM big.LITTLE systems, big cores are mainly used for performance-critical workloads or workloads with Instruction Level Parallelism (ILP). Executing other kinds of workloads on them would not improve the performance significantly, but would significantly increase energy consumption. To build an efficient AMP scheduler, we need to predict which threads are suitable for which kind of core.

\textbf{\textit{Thread criticality:}} Executing a single thread of a workload faster does not always translate into better performance of the whole workload. If the threads of an application are unbalanced or are executed at different speeds, e.g.~because different threads run on different types of cores, the application will run only as fast as its slowest or most critical thread (the thread that blocks most of the other threads). A good AMP scheduler would accelerate these threads as much as possible, regardless of core sensitivity. 

\textbf{\textit{Fairness:}} In multiprogrammed workloads, accelerating an individual application in isolation is not enough if it penalizes other applications. Ideally, we need to have \emph{fair scheduling} that will balance the negative impact of resource sharing uniformly across all applications. In homogeneous sytems, this is easily achieved by giving each application a fixed-size time slice on a CPU in a round-robin way. AMPs make this simple solution unworkable. The same amount of CPU time on different core types results in completely different amounts of work performed, due to difference in performance of each core.


Prior research~\cite{han2018multicore,chronaki2017task,joao2012bottleneck,suleman2009accelerating,du2013criticality} has explored bottleneck and critical section acceleration, others have examined fairness~\cite{zahedi2018amdahl,wang2016rebudget,van2012scheduling,li2009efficient,li2007efficient}, or core sensitivity~\cite{cao2012yin,kumar2004single,becchi2006dynamic}. More recent studies~\cite{kim2018exploring,kim2016fairness,saez2012leveraging,van2013fairness,joao2013utility} have improved on previous work by optimizing for multiple factors. Such schedulers are good only for specific kinds of workloads. Only one previous work, WASH~\cite{jibaja2016portable}, can handle general workloads composed of multiple single- or multi-threaded applications with potentially unbalanced threads, and with a total number of threads that may be higher than the number of cores. While a significant step forward, WASH only controls core affinity and does so through a very fuzzy heuristic. The former means that we cannot handle core allocation and thread dispatching in a holistic way to speed up the most critical threads. The latter means that WASH has only limited control over which threads run where, leaving much of the actual decision making to the underlying Linux CFS scheduler.

\begin{figure}
\centering
\includegraphics[scale = 0.25]{figures/me.pdf}
\caption{Motivating Example: Multi-threaded multiprogrammed workload on asymmetric multicore processors with one big core $P_b$ and one little core $P_l$. Controlling only core affinity results in suboptimal scheduling decisions.}
\label{me}
\end{figure} 

In this paper, we introduce COLAB, an OS scheduling policy for asymmetric multicore processors that makes coordinated decisions targetting all three main factors in thread scheduling - core sensitivity, thread criticallity and fairness. Our scheduler uses three collaborating heuristics to drive decisions about mapping threads to cores, each of the heuristics foucing primarly on optimisation of one of the factors. Collectively, these multi-factor heuristics result in better thread schedules compared to the Linux and WASH schedulers, therefore improving both performance and energy consumption. %ing decisions.
%We integrated COLAB in the Linux scheduler module, replacing the default CFS policy for all application threads.

The main contributions of our work are:
\begin{itemize}
\item  We present the design of a novel COLAB AMP-aware OS scheduler that targets general multi-threaded multiprogrammed workloads. The scheduler is based on a set of novel collaborative heuristics for addresing core sensitivity, thread criticality, fairness \emph{and} energy efficiency.
\item We present an implementation of the COLAB scheduler both on a real chip and in the simulation settings.
\item We evaluate the effectiveness of the COLAB scheduler on a range of standard workloads, demonstrating improvements of up to 25\% and 21\% (11\% and 5\% on the average) in the turnaround time compared to the Linux CFS and WASH schedulers in the GEM5 simulator and up to 27\% and 10\% performance gain (together with 5\% energy saving) compared to the ARM GTS and WASH scheduler on a real big.LITTLE development board.
  %
%\item Up to 25\% and 21\% lower turnaround time, 11\% and 5\% on average, compared to the default Linux CFS and WASH scheduler on GEM5. 
%\item Up to 27\% and 10\% performance gain, together with 5\% energy saving, compared to the default ARM GTS and WASH scheduler on HiHope Hikey 970.
\end{itemize}

\subsubsection*{Motivating Example} To demonstrate the problem, consider the example shown in Figure~\ref{me}, with an AMP system that has a high performance big core, $P_b$, and a low performance little core, $P_l$. Three applications are being executed - $\alpha$ and $\beta$ that have two threads, and $\gamma$ that is single threaded. 
The first thread of each application, $\alpha_1$ and $\beta_1$, blocks the second thread of their application, $\alpha_2$ and $\beta_2$, respectively. %$\gamma$ is a single-threaded application. 
$\alpha_1$ and $\gamma$ enjoy a high speedup when executed on $P_b$. WASH~\cite{jibaja2016portable}, the existing state-of-the-art multi-factor heuristic, would be inclined to assign the high speedup thread and the two blocking threads to the big core. The thread selector of $P_b$ has no information about the criticality of the threads assigned to it, so the order of execution depends on the underlying Linux scheduler.
A much better solution is possible if we control both core allocation and thread selection in a coordinated, AMP-aware way. In this case, we map the two threads that benefit the most from the big core, $\gamma$ and $\alpha_1$, to $P_b$, while we map the other bottleneck thread, $\beta_1$, to $P_l$. This will not impact the overall performance of $\beta$. The thread selector knows $\beta_1$ is a bottleneck thread and executes it immediately. So, what we lose in execution speed for $\beta_1$, we gain in not having to wait for CPU time. Similarly, this coordinated policy guarantees that $\alpha_1$ will be given priority over $\gamma$. 

%Based on the multi-factor mixed model \cite{jibaja2016portable}, either the high speedup thread ($\gamma$) or blocking threads ($\alpha_1,\beta_1$) will have affinity on $P_b$ -{\it local-optimal decision}. When the thread selector on $P_b$ is invoked, no more guideline can be obtained from the thread affinity provided by the mixed model - {\it loss of information with mixed priority}. Instead, a better solution can be provided in a multi-factor decentralized model by only mapping the two high speedup threads ($\gamma,\alpha_1$) to $P_b$ and keeping the other blocking thread $\beta_1$ in $P_l$ - {\it global-aware decision}. Then the thread selector invoked by both $P_b$ and $P_l$ guided through the blocking priority, will select and accelerate bottleneck threads $\alpha_1,\beta_1$ locally and simultaneously without waiting each other - {\it sufficient information with precise priority}.  

%both of them will get high priority and be affiliated on the big core. Thus, some of those high priority threads need to be waited in the runqueue of $p_b$. Further, when $p_b$ is ready to select the next task, it may select a $t_s$ first instead of a $t_b$ as the original bottleneck priority has lost after setup its affinity. In brief, the core allocator made the first {\it selfish} greedy decision to enqueue those high priority threads on big core without leaving a better solution space for thread selector. Second, the thread selector lose original information and cannot guarantee to make a good decision. With efficient preemption mechanism, $p_l$ can select a $t_b$ even from $p_b$'s runqueue, but this leads to additional overhead from migration. If the core allocator and thread selector can be collaborated, a trivial better solution in this example can be $p_b$ running \{$t_s \cap t_b$\} whilst $p_l$ running \{$t_b$\}. 

%The first problem is obvious as in runtime scheduling, every change in a current decision will influence the future solution space and there is no way to predict it especially for asymmetric multicore processors with multiple multi-thread workloads.  
%To demonstrate the second problem, scheduling a ready thread from a run-queue in big core to be executed in a ready little core can result in an optimal solution from a greedy point of view, or we say a {\it current} optimal solution. But if this thread will only run on the little core for a tidy bit of time and then move back to a big core by preemption when the big core is ready, the overall runtime overhead from additional migrations may totally defeat the benefit from the hard and useless work on the little core. A high level motivating example is shown in Fig \ref{mt}, where $\mathcal{T}$ is a high priority thread which just ranked in the second position under the current running thread on big core's run-queue. By global scheduling, a ready little core will decide to run this high priority thread $\mathcal{T}$ and migrate it from big core's run-queue. While the problem happens if the big core finish its current thread right after the migration issued by the little core, as the big core will decide to run $\mathcal{T}$ and move it back by preemption. Those frequent runtime selections and migrations do lower the performance in this case.

%We evaluated our policy on multiple big.LITTLE-like simulated systems running 36 distinct workloads, each workload being a random selection of PARSEC3.0, SPLASH2, and SPEC CPU2006 benchmarks. 
%In almost all cases, COLAB was able to improve both turnaround time and throughput compared to the state-of-the-art and the Linux default. In the best case, turnaround time was 25\% less under our policy than under WASH and CFS.
%In brief, our proposed framework equipped with a multi-factor {\it decentralized} model - different runtime factors are distributed to be mainly addressed by different functional units of the scheduler, including core allocator, thread selector and preemption trigger. Decisions in each functional unit are directly guided by priorities from sufficient runtime without losing information through a mixed central model. Those decisions only aim to benefit each of their targeting factors and avoid making trouble for each other, so neither greedy nor local-optimal from a multi-factor point of view. Then all those sub-decisions from different functional units in scheduler dynamically collaborate together to result in overall smarter scheduling decisions during runtime. 
%The remainder of this paper is presented as follows: Section 2 describes the background and related work. Section 3 presents our multi-factor collaborative model. We describe its implementation and analyze its operation in Section 4. We evaluate the scheduling police in Section 5, and we summarize and conclude the paper in Section 6.  
