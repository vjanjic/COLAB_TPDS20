\section{System Overview}
We provide a high level COLAB system overview as shown in Figure~\ref{figure:f1}. The system is divided into a runtime scheduling process and an offline modelling processes. The runtime scheduler is built inside OS kernel and composed of i) a core allocator to handle fairness and core sensitivity; ii) a thread selector to achieve bottleneck acceleration; and, iii) machine learning based runtime models, which predicate speedup and power consumption of threads on heterogeneous cores. The offline modelling processing is applied to construct runtime models by offline training. 

The main novelty of COLAB scheduler is that it can handle multiple runtime factors (core sensitivity, bottleneck acceleration and fairness) in a collaborative way to achieve high system performance and energy efficiency. To easy understand the runtime collaboration, we first analyze the performance impact of multiple runtime factors and their relationship with different scheduler components. We then discuss how to build the scheduler which addresses these performance problems in a coordinated way. After that, we present the detailed design and implementation of the proposed COLAB scheduler, beginning from the supporting offline modelling process to the runtime scheduling process. 

\begin{figure}
\centering
\includegraphics[scale=0.27]{figures/overview.png}
\caption{System Overview}
\label{figure:f1}
\end{figure} 

\begin{figure}
\centering
\includegraphics[scale=0.4]{figures/mfa.pdf}
\caption{A diagram of Performance Factors and Relationships with Scheduling Functions}
\label{figure:f11}
\end{figure} 

\section{Runtime Analysis}
Figure~\ref{figure:f11} shows the relationship between runtime performance factors and the scheduler components that address them. 
In order to achieve runtime collaboration, both the core allocator and the thread selector share information and account for all measured performance factors, including core sensitivity, bottleneck acceleration and fairness. In this section, we first analyse the relationships and then discuss the collaboration. 
% as illustrated below:
%We describe them below by discussing each element. 
%\begin{itemize}
\subsection{Runtime Factors Analysis}
\textbf{\textit{Core Allocator:}}
AMP-aware Core allocators are mainly directed by the core sensitivity factor -- migrating a high speedup thread (with a large differential between big and little core execution time) from  a little core to execute on a big core will generally provide more benefit than migrating a low speedup thread. 
However this heuristic is overly simplistic. Issues are revealed when the bottleneck factor is considered simultaneously on multiprogrammed workloads. Previous approaches \cite{jibaja2016portable} simply combine the calculation from  bottleneck acceleration and predicted speedup together, but this can result in suboptimal scheduling decisions -- both locking threads and high speedup threads may be accumulating in the runqueues of big cores as described in the motivating example. More intelligent core allocation decisions can be made by avoiding a simple combination of bottleneck acceleration and speedup -- the overall schedule can benefit from a more collaborative execution environment where big cores focus on high speedup bottleneck threads, and little cores handle other low speedup bottlenecked threads without additional migration.
%migrating threads with a lower predicted speedup, but which are blocking other threads.  
Furthermore, core allocators attempt to achieve relative fairness on AMPs by efficiently sharing heterogeneous hardware and avoiding idle resources as much as possible. 
Simply mapping ready threads uniformly between different type of cores can not achieve true load-balancing -- the number of ready threads prioritized on different type of core is different and thus a hierarchical allocation should be applied to guarantee overall fairness, which avoids the need to frequently migrate threads to empty runqueues. 

\textbf{\textit{Thread Selector:}}
The \textit{thread selector} makes the final decisions on which thread will be executed during runtime. It is usually the responsibility of the thread selector to avoid bottlenecking by thread blocking. In a multi-thread multiprogrammed environment, multiple bottleneck threads from different programs may need to be accelerated simultaneously with constrained hardware resources. Instead of simply detecting the bottleneck threads and assigning all of them to big cores, as previous bottleneck acceleration schedulers do~\cite{jibaja2016portable,joao2013utility,joao2012bottleneck}, the thread selector needs to make collaborative decisions -- ideally, both big cores and little cores select bottlenecks to run simultaneously.
Core sensitivity is usually unimportant to the thread selector -- whether a thread can enjoy a high speedup from a big core compared with a little core is unrelated to which runqueue it is on, or came from. Therefore the thread selector should separate thread priority caused by core sensitivity and solely base decisions on bottleneck acceleration. One exception is that when the runqueue of a big core is empty and the thread selector is invoked -- the speedup factors from core sensitivity of ready threads should be considered only in this case. Big cores may even preempt the execution of little cores when necessary.  
The final concern of thread selector concerns fairness. Scaling time slice of threads by updating the time interval of thread selector has been shown to efficiently guarantee the equal progress \cite{van2013fairness} in multi-threaded single-program workloads and achieve fairness. 
%In single-threaded multiprogrammed scenarios, complicated fairness formulation \cite{kim2018exploring} has been proposed to guide the thread selector for more precise decisions. 
Problems occur when targeting multi-threaded multi-programmed workloads. Simply keeping a thread-level equal progress is not enough to guarantee the multi-application level fairness -- the thread selector should ensure the whole workload is in equal progress without penalizing any individual application. In fact,  multi-bottleneck acceleration by both big and little cores does provide an opportunity for this - the thread selector makes the best attempt to keep fairness on all applications by accelerating bottlenecks from all of them and as soon as possible.


%\textbf{\textit{Fairness:}} Fairness is critical for system performance. A good schedule achieving relative fairness on AMPs should efficiently share heterogeneous hardware and avoid idle resource as much as possible. To achieve the expected relative equal-progress between threads and load-balancing between cores, the core allocator should map ready threads relative uniformly,avoiding the need to frequently migrate threads to empty runqueues. The design of the preemption triggering mechanism  also affects fairness, as it determines the maximum length of a time slice.


%The preemption triggering mechanism of thread selector is also related with the fairness factor as it determines the slice of a task running each time - a thread currently running on big cores should have relatively shorter time slice against its running on little cores to result in similar progress.
%The preemption should be triggered and the thread selector should be invoked less frequent on little cores than on big cores     

%\textbf{\textit{Core Sensitivity:}} Previous approaches considered core sensitivity and predicated speedup of threads but can result in poor scheduling decisions on multi-threaded, multi-programmed co-execution workloads. Selecting threads simply by predicted speedup on asymmetric cores may not lead to a overall good solution --  for instance, a high speedup thread detected on a little core could benefit by migrating to a big core, but is not blocking a significant number of other threads. The overall schedule can benefit more by migrating threads with a lower predicted speedup, but which are blocking other threads. Thus, we find priority from core sensitivity should be mainly addressed by the core allocator and be independent from the thread selector. 
%For instance, a thread with high predicted speedup should have more opportunities to be allocated and executed on a high-performance big core compared to a low predicted speedup thread. Higher speedup and more sensitive on big core does not indicate any {\it emergency} to execute this threads amount all other ready threads during the runtime. 
%No priority should be given to high-speedup threads for a thread selector, only if in a special case when a big core finishes its current task with an empty runqueue and all other big cores hold empty runqueue as well - so it needs to globally select a ready thread from a runqueue of little core and then relative higher speedup threads are better candidates. This special case won't even appear in large-scale parallel workloads scenarios where the number of co-executing programs is at least greater than the number of cores - there will always be waiting threads in a big core's runqueue based on fairness core allocation as no data-dependence between threads from different programs. 

%\textbf{\textit{Bottleneck Acceleration:}} Threads from multi-threaded programs holding contended locks and blocking other parallel threads should be identified as bottleneck and be executed as soon as possible. In large-scale workloads scenarios where multi-threaded multi-program are co-executing on limited AMPs resources and multiple blocking threads waiting to be executed simultaneously, the thread selector should take the main duty to select those bottlenecks and run the corresponding critical code segments in an efficient way without confused by the priority from core sensitivity. In detail, the thread selector triggered by big cores should always give higher priorities for a thread with higher blocking level against another thread with higher speedup level and lower bottleneck. It may preempt the running threads on little cores to accelerate them when suited. The thread selector triggered by little cores should also intend to select relative high blocking threads and accelerate them locally first, instead of trivially migrating it to big cores and waiting to be actual accelerated there.  

\begin{figure}
\centering
\includegraphics[scale=0.5]{figures/COLAB_M.pdf}
\caption{Coordinated Runtime Scheduling by Multi-factor Collaboration}
\label{figure:f2}
\end{figure} 

\begin{figure*}
\centering
\includegraphics[scale=0.25]{figures/arm_model.pdf}
\caption{ARMv8 Performance and Energy Modelling. \fixme{PP: This looks like a screenshot. Also (a) does not communicate much information. In (c) is this actually Energy Consumption (as stated on the y-axis) or Energy Efficiency Scaling between big and little cores (which is what we use later)? If it's energy consumption, for which core?}}
\label{arm_model}
\end{figure*}
\subsection{Runtime Collaboration}
To address the problems detailed above, we designed a coordinated multi-factor scheduler in which the core allocator and the thread selector collaborate to achieve high performance and high fairness, when compared to %the state-of-the-art mixed multi-factor evaluator in 
WASH \cite{jibaja2016portable}.
% VJ " We mentioned quite a few times already that WASH is state-of-the-art mixed multi-factor thing...
%While the way we define whether a thread's relative speedup value is high or low is similar as in WASH based on hardware configuration - if we have same number of big and little cores, then we rank all ready threads based on its relative speedup and view the first half as high speedup value. 
The flowchart of our model is shown in Figure~\ref{figure:f2}. 
Collaboration is facilitated by periodically labeling ready threads in two different categories, based on runtime models of speedup prediction and bottleneck identification: 
%\begin{itemize}

\textbf{\textit{Labels for Core Allocation:}}
Threads with high predicted speedup between big and little cores will be labeled as high priority on big cores. Threads with both low predicted speedup and blocking levels -- non-critical threads -- will obtain high priority on little cores (and low priority on big cores). Remaining threads obtain equal priority on either big or little cores -- these threads can then be allocated freely to balance the load of cores.

\textbf{\textit{Labels for Thread Selection:}}
Threads with high blocking level will be labeled as high priority on local thread selection. The same priority will be given on these blocking threads whether the issuing cores are big or little, so the labels of thread selection do not distinguish the type of cores.The label nevertheless records the type of the current core -- threads always have priority to be selected by the same type of cores if there exists a core of the same type with an empty runqueue. Running threads on little cores are also labeled as they may be preempted to migrate and execute on big cores when suited, but running threads will never have priority over waiting ready threads. 


%The thread labels should be not only be based on the relative ranking between threads, but also on boundary conditions targeting different hardware resource and experimental environments. 
%We setup empirical boundaries for speedup and blocking: (1) No thread is ranked as high-speedup if its predicted speedup value is less than an empirical architecture-specific boundary, calculated using relative frequency between big and little cores. 

%For instance, if the big cores' frequency is 2x faster than the little cores and ready threads get around 1.5x predicted speedup, this likely means threads haven't be executed long enough to get a more precise speedup value and we should not bind or migrate any thread at this step. 
%(2) Threads will not be ranked as high-blocking if the rate of their blocking time and life time are less than a relative boundary or the life time of them are less than a starting point - For instance, no thread should be ranked as relative high-blocking during the initial time periods of experiments.   
%It is mainly benefit in a mixed workloads scenario where we co-execute single and multi-threaded multiprogrammed - a non-computing-intensive code segment from a single thread program can neither enjoy good performance gain on big cores nor block others. So we should better keep it in a little core and let other blocking threads to be executed before it. 

After the labeling process, fairness, core sensitivity and bottleneck acceleration are represented by labels on threads can be handled by either the core allocator or the thread selector or both together. Based on this coordinated model, the core allocator and thread selector handle different priorities queues from the set of ready threads -- their decisions are not greedy on a mixed multi-factor ranking like WASH, rather provide a collaborative schedule.
Another important issue handled by the collaborative multi-factor model is to ensure equal-progress of threads as shown in the upper-right corner of Figure~\ref{figure:f2}. Instead of interfering with the priority and decisions of thread selection, we achieve equal progress in threads by our scaled time slice approach, based on the predicted speedup value of threads running on big cores. The slices of threads on big cores are relative shorter than on little cores. The thread selection function is triggered more often to swap executing threads on big cores, which guarantees the relative equal-progress of threads executed on all cores.
The runtime model periodically extracts the performance counters, which represents the current execution environment of multi-threaded multi-programmed workloads on the AMPs. The model then computes the updated runtime factors, including the predicated speedup value and blocking counts. This information is attached to the threads and reported back to the multi-factor labeler for next round. We present our runtime implementations in the section below. 



\section{COLAB Scheduler Design and Implementation}
This section explores the design and implementation of COLAB inside the Linux kernel. We describe our performance and energy models, our modifications of the kernel to support COLAB, and finally the scheduling algorithm itself, including its runtime overheads.

\subsection{Offline Performance and Energy Modelling}
The decisions of our Core Allocator are primarily driven by core sensitivity. To predict whether a thread is core sensitive or not, we develop a machine learned performance model similar to the ones used in previous works in this area~\cite{van2013fairness,jibaja2016portable,saez2012leveraging}. The model is constructed offline once and is kept purposefully simple, a linear regression model with seven parameters, to minimize the runtime overhead. 

%Predicting the relative speedup of each thread between different core types is central for any scheduler targeting AMPs. Our prediction uses a machine learning approach by offline training and online estimation as shown in Figure~\ref{arm_model}. This is a common approach in previous works \cite{van2013fairness,jibaja2016portable,saez2012leveraging}. 

%The abstract offline PMU selection process is shown in Fig.~\ref{arm_model}(a). 

The training data we collect are execution time, energy consumption, and event counts from all performance monitor units (PMU). We run each training application in isolation under two different configurations, first on the little core cluster and then the big core cluster of our evaluation system. Since we only have access to seven PMUs at any given moment, we repeatedly execute each application while collecting different PMUs, until we have reliable information for all of them. At the end of this process, we have performance event counts, performance scaling between big and little cores, and energy scaling between big and little cores for 14 programs. Through Principal Component Analysis (PCA)~\cite{witten2016data}, we select the six performance events that correlate the most with performance and energy scaling. Finally, we use linear regression to associate the six selected events, as well as the clock cycle count, with performance and energy scaling. This results in the two models shown in Table~\ref{arm_model}(b).

%To construct the training set, we run all applications in single-program mode with two symmetric configurations, using either only little cores (4 Cortex-A53) or only big cores (4 Cortex-A73). We first record common performance monitor units (PMUs) used for both Cortex-A73 and Cortex-A53, together with each thread's relative speedup and energy consumption. We ignore the PMUs for which their value keep to be zero, such as \texttt{SW\_INCR} (Instruction architecturally executed, Condition code check pass, software increment). 

%Since the targeting ARMv8 architecture only contains 7 hardware register to support PMUs, we do not have access to all PMUs simultaneously. We apply Principal Component Analysis (PCA) technique \cite{witten2016data} to select a 6-PMU-combination with the largest effect on speedup and power modeling. For example, it is easy to know that the performance and energy will be more dominated by the total data movement than some branch behaviours. As a result, although \texttt{BR\_IMMED\_RETIRED} is a meaningful PMU which counts all immediate branch instructions that are architecturally executed, but it is not selected after the PCA process as there are more important PMUs such as \texttt{L1D\_CACHE\_REFILL} (L1 data cache refill) and \texttt{L2\_CACHE} (L2 data cache access). We then normalize all counters to the number of clock cycles and use linear regression to build the final model. The clock cycle number is stable recorded by a specialized register on the test ARMv8 development board. The trained PMU-based performance and energy models are shown in Table~\ref{arm_model}(b).

We validate the accuracy of our trained models on each benchmark as shown in Fig.~\ref{arm_model}(c). The average error across all benchmarks is around 7\% for performance scaling prediction and around 1.5\% for energy scaling prediction. \fixme{PP: Can we say something about why we are so wrong for lu\_cb?}


\begin{comment}
\begin{table}
  \caption{Selected GEM5 performance counters and the Speedup Model}
  \center
  \label{pca_gem5_sp}
   \scalebox{0.8}{
   \begin{tabular}{p{0.8cm} | p{4.8cm} | p {3.8cm} c c c}
  \hline
     Index&  Name& Description \cite{binkert2011gem5} \\
    \hline
     A: & fp\_regfile\_writes & \# integer regfile writes\\
     B: & fetch.Branches & \# branches encountered\\
     C: & rename.SQFullEvents & \# SQ-full blocks\\
     D: & quiesceCycles & \# interrupt waiting cycles\\
     E: & dcache.tags.tagsinuse & \# tags of dcache in use\\
     F: & fetch.IcacheWaitRetryStallCycles & \# MSHR-full stall cycles\\
     \hline
     G: & commit.committedInsts & \# instructions committed\\
     \hline
     \multicolumn{3}{c}{Linear predictive speedup model}\\
     \hline
    \multicolumn{3}{c}{2.6109+((0.0018*-0.185A)+(0.0259*0.187B)+}
    \\
    \multicolumn{3}{c}{(0.1047*0.194C)+(-0.023*0.238D)+(0.0492*-0.299E)+(-0.1388*-0.227F))/G}\\
 \hline
  \end{tabular}}
\end{table}
\end{comment}

\subsection{Runtime Supporting Techniques}
\textbf{\textit{Bottleneck Identification:}}
On modern Linux systems thread synchronization primitives are almost always implemented on top of kernel futexes, regardless of the threading library used. Futex-based mechanisms use a single atomic instruction in user space to acquire or release the futex, if it is uncontested. Otherwise, it calls the kernel which forces the thread to sleep or wakes up sleeping threads respectively.
This means that monitoring the blocking patterns between threads requires instrumenting only this interface. Right before an active thread starts waiting on a futex, in \texttt{futex\_wait\_queue\_me()} and \texttt{futex\_lock\_pi()}, we record the current time and store it in the thread's \texttt{task\_struct}. We mirror this with code right before a waiting task is woken up, in \texttt{wake\_futex()} and \texttt{wake\_futex\_pi()}. At this point we calculate the length of waiting time for the thread and we accumulate it in a field of the \texttt{task\_struct} of the thread releasing the futex. This enables us to measure the cumulative time each thread has caused other threads to wait. This is our thread criticality metric for the rest of the paper.

%We instrument this interface to monitor the blocking patterns between threads. This gives us a convenient single point for monitoring blocking patterns between threads. We first add code in \texttt{futex\_wait\_queue\_me()} and \texttt{futex\_lock\_pi()}, right before the active thread starts waiting on a futex. We record the current time and store it in the \texttt{task\_struct} of the thread. We then insert code in \texttt{wake\_futex()} and \texttt{wake\_futex\_pi()}, right before the waiting task is woken up by the thread releasing the futex. There we calculate the length of the waiting period and we accumulate it in the \texttt{task\_struct} of the thread releasing the futex. This way we are able to measure the cumulative time each thread has caused other threads to wait. We use this as our metric of thread criticality for the rest of the paper.

\textbf{\textit {Speedup based Scale-slice Preemption:}} The default preemption mechanism of Linux is triggered every time a new task in enqueued at which point it checks whether the virtual runtime {\it vruntime} of the incoming task is significantly lower than that of the running task. If this is the case, sharing the processing time fairly requires preempting the running task. While our approach keeps this mechanism intact, we modify vruntime so that equal vruntimes represent equal progress in an AMP system instead of just equal runtime. We do this in the default preemption function \texttt{wakeup\_preempt\_entity()}. If the triggering core is a big one, then the vruntime of the task is divided by the model predicted speedup for the task. This is equivalent to predicting the vruntime required to achieve the same progress on a little core.

%Although we implement our scheduler on Linux kernel by fully re-writing both the core allocator and thread selector, the underlying preemption mechanism of Linux is applying the virtual runtime {\it vruntime} in CFS with red-black tree data structure - whenever a new task is enqueued, a preemption wake-up function is invoked to check whether the new coming task should preempt the current task by computing the difference in vruntime and comparing with a boundary. 
%To achieve equal-progress on AMPs, threads running on different types of cores should have different time slices instead of trying to achieve complete fairness on time. We update the default preemption function \texttt{wakeup\_preempt\_entity()} in Linux. To ensure relative equal progress, we apply our runtime speedup model to update the vruntime of the task by dividing it by the its speedup value if the triggering core is a big core. %The ensures relative equal progress.

\subsection{Scheduling Algorithm Implementation}
\begin{figure}[!t]
\label{alg:1}
\begin{algorithmic}[]
\STATE \textbf{\_core\_alloctor\_(thread\_struct\ $t$)}:
%\FOR {$t : ready\_threads$}
%\IF {t->high\_speedup() \& t->high\_block()}
%\STATE \textbf{if} foo

%\IF {t.high\_speedup}
%\RETURN rr\_allocator\_(big\_cores)
%\ENDIF
\STATE \ \ \textbf{if} t.high\_speedup
\STATE \ \ \ \ \textbf{return} rr\_allocator\_(big\_cores)
\STATE \ \ \textbf{if} {t.low\_speedup \& t.low\_block}
\STATE \ \ \ \ \textbf{return} rr\_allocator\_(little\_cores)

\STATE \ \ \textbf{else return} rr\_allocator\_(cores)
\STATE
\STATE \textbf{\_thread\_selector\_(core\_struct\ $c$)}:
%\IF {c->cur \& !(preempt_wakeup)}
\STATE \ \ \textbf{if}  !empty(c.rq)
\STATE \ \ \ \ \textbf{return} max\_block\_(c.rq)
\STATE \ \ \textbf{if} !empty(c.sched\_domain.rq)
\STATE \ \ \ \ \textbf{return} max\_block\_(c.sched\_domain.rq)
\STATE \ \ \textbf{if} c.cpu\_mask == big
%\FOR {t : little\_core->cur}
%\STATE candidates.enqueue(t)
%\IF {!empty(candidates)}
\STATE \ \ \ \ \textbf{return}  max\_block\_(c.sched\_domain\_little.cur)
%\ENDFOR
%\ENDIF
%\FOR {t : other\_little\_core->rq}
%\STATE candidates.enqueue(sorted\_speedup(t))
%\ENDFOR
%\IF {!empty(candidates)}
%\RETURN  max\_block\_(candidates)
%\ENDIF
%\ENDIF
%\FOR {t : other\_core->rq}
%\STATE candidates.enqueue(reversed\_speedup(t))
%\ENDFOR
\STATE \ \ \textbf{else return} idle
\end{algorithmic}
\caption{Collaborative Multi-factor Scheduler Algorithm}
\end{figure}

\noindent
We implement our scheduling algorithm, shown in Figure~\ref{alg:1}, by overriding the default Linux task selector \texttt{pick\_next\_task\_fair()} and core allocator \texttt{select\_task\_rq\_fair()}.
In line with standard Linux notation, we use $rq$ and $cur$ to represent runqueue and the current task of a core, respectively. In the following paragraphs, we describe the two main functions as well as an energy-aware extension.

\textbf{\textit{Hierarchical Core Allocator:}}
The core allocator's role is to assign newly ready threads to core runqueues. Threads become ready to be executed, either when they are woken or spawned. Our implementation first assigns threads to clusters, the big core or the little core one, based on the speedup and blocking labels selected for each thread at runtime. Threads labeled as high speedup are assigned to big core cluster, while low speedup and low blocking are assigned to the little core cluster. All other ready threads, either low speedup and high blocking or average speedup and low blocking, are assigned to both clusters. Finally, a hierarchical round-robin mechanism \texttt{rr\_allocator\_()} chooses one core within the selected cluster for the thread. This helps keep the system load balanced. 

%When a spawned or woken thread is ready to be executed, the core allocator will be invoked to assign this thread to a core's runqueue. To achieve relative load balancing and consider the influence from the core sensitivity factor, we involve a hierarchical round-robin mechanism \texttt{rr\_allocator\_()}. Indicated by the speedup and blocking labels, threads are allocated to different core groups. Threads with high speedup will be round-robin assigned in big core clusters. Low speedup and low blocking threads will only be assigned in little core clusters. 
%%A technical issue here is we need to distinguish threads which are part of a multi-thread program from from single-thread programs - these single-thread program threads will definitely not block others and may not have a high speedup but we should nevertheless not bind them to little cores. They can be easily detected by the allocator by checking whether there are ready threads whose group pid (t->tgid) equals to any others (line 5). 
%Remaining ready threads (usually with average speedup level and little blocking) will be relatively equally allocated to both core types by \texttt{rr\_allocator\_()}. This final round-robin decision helps to keep both core clusters equally occupied and load balanced. 

\textbf{\textit{Biased-global Thread Selector:}}
The thread selector's primary objective is to accelerate the most critical/blocking threads as soon as possible. The selector always searches for a ready thread from the local runqueue first, preferably a high blocking one. If there are no ready threads and migration is beneficial, the core triggers the migration of a candidate thread waiting in another runqueue. The highest blocking thread will be selected for migration.
To reduce the overhead of accessing state in other runqueues, we follow the same principle as the default Linux CFS scheduler, returning the best candidate thread from the local core group first.
Further, we allow a big core to select and preempt a running thread on a little core to accelerate it. Big cores are allowed to go idle only when there are no ready threads left. The converse, little cores preempting big cores is not allowed. 
%In summary, the thread selector can still access all other runqueues when necessary, but it is biased to access neighbouring ones first. 
The equal-progress for achieving fairness is addressed by the scale-slice preemption checker described earlier: we give each thread a maximum time slice according to its expected performance on a little core.



